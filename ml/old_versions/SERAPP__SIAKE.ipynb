{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWH_tr0rGTiT"
      },
      "source": [
        "# Projet SER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfqaoNRXGMwe"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas scikit-learn\n",
        "!pip install soundfile\n",
        "!pip install librosa\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_bTdm2BlGMwg"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'seaborn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msignal\u001b[39;00m \u001b[39mimport\u001b[39;00m find_peaks\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msoundfile\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msf\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix, classification_report, precision_score, recall_score\n",
        "import librosa\n",
        "from scipy.signal import find_peaks\n",
        "import soundfile as sf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2EqIjw1GPC0"
      },
      "source": [
        "## Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lVE6sy4IKaM"
      },
      "source": [
        "### From local path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlKDnjZsGMwg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def collect_data_ravdess(audio_path):\n",
        "    data = pd.DataFrame()\n",
        "\n",
        "    for file in os.listdir(audio_path):\n",
        "        if file.endswith(\".wav\"):\n",
        "            filepath = os.path.join(audio_path, file)\n",
        "            label = file.split(\"-\")[2]  # Extracting emotion label from filename\n",
        "\n",
        "            # Append the filepath and label to the DataFrame\n",
        "            data = data.append({\"filepath\": filepath, \"label\": label}, ignore_index=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH9G-lWfIbTv"
      },
      "source": [
        "### From kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lm9nFbxOlTB8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPQJgXsVlfgH",
        "outputId": "53540165-f75b-4170-c35e-95d195999b0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: /Users/siakesophie/.kaggle: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Après la création de `~/.kaggle` dans votre ordinateur, il faut ouvrir le fichier créer suite à cette commande `!touch ~/.kaggle/kaggle.json` et y mettre vos identifiant kaggle. Cela est nécessaire pour avoir accès au datasets de kaggle.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BNB8NxL9mzc7"
      },
      "outputs": [],
      "source": [
        "!echo '{\"username\":\"siakesophie\",\"key\":\"a14396222d9d4576cce4412a57731f71\"}' > ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_token = {\"username\":\"siakesophie\",\"key\":\"a14396222d9d4576cce4412a57731f71\"}\n",
        "\n",
        "with open('/Users/siakesophie/.kaggle/kaggle.json', 'w') as file:\n",
        "        json.dump(api_token, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                           title                                  size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------  ------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "uwrfkaggler/ravdess-emotional-speech-audio    RAVDESS Emotional speech audio        429MB  2019-01-19 18:28:31          38697        474  0.875            \n",
            "uwrfkaggler/ravdess-emotional-song-audio      RAVDESS Emotional song audio          456MB  2019-01-19 21:01:49           1939         39  0.875            \n",
            "dmitrybabko/speech-emotion-recognition-en     Speech Emotion Recognition (en)       987MB  2021-01-25 12:59:50           8542         93  0.875            \n",
            "kartik2khandelwal/speech-emotion-dataset      RAVDESS as .csv                       763KB  2021-09-15 13:12:51            693         17  0.5882353        \n",
            "uwrfkaggler/ravdess-facial-landmark-tracking  RAVDESS Facial Landmark Tracking      445MB  2019-09-10 18:05:59            509         31  0.85294116       \n",
            "uldisvalainis/audio-emotions                  Audio emotions                          1GB  2020-06-09 12:56:17           2138         33  0.75             \n",
            "cracc97/features                              MFCCs for Speech Emotion Recognition   37MB  2021-02-13 18:05:08            650         14  0.88235295       \n",
            "dejolilandry/ravdess                          Ravdess                               215MB  2021-12-26 04:25:21             78          1  0.4375           \n",
            "preethikurra/ravdess-tess                     RAVDESS, TESS                         428MB  2023-03-23 19:48:49             29          1  0.625            \n",
            "borameister/ravdessfractals                   RAVDESS-Fractals                        7MB  2019-09-30 13:50:17             23          1  0.1875           \n",
            "mostafaabdlhamed/speech-signal-features       speech recognition features           727MB  2023-07-01 03:39:46             83         10  0.5625           \n",
            "adrivg/ravdess-emotional-speech-video         RAVDESS Emotional speech video         12GB  2023-02-05 19:13:09            123          2  0.375            \n",
            "divyanshudata/ravdess                         RAVDESS                                 9MB  2023-11-09 17:55:38              0          1  0.125            \n",
            "tariqblecher/ravdess-8k                       ravdess_8k                             43MB  2022-06-25 08:55:24             34          1  0.5              \n",
            "deepakk6/ravdess-data                         ravdess data                           75MB  2022-05-08 17:32:15             16          2  0.125            \n",
            "bhuviranga/basic-audio-dataset                Basic Audio Dataset                     9MB  2023-08-01 10:35:22              9          2  0.8125           \n",
            "borameister/ravdess-cochleagrams              RAVDESS - Cochleagrams                 30MB  2019-09-30 09:07:22              9          1  0.125            \n",
            "borameister/ravdessmfccs                      RAVDESS-MFCCs                          16MB  2019-09-30 13:51:18             12          1  0.125            \n",
            "borameister/ravdessspectrograms               RAVDESS-Spectrograms                  127MB  2019-09-30 13:51:56             22          1  0.125            \n",
            "borameister/ravdesslt                         RAVDESS-LT                             55MB  2019-08-08 21:22:19             11          1  0.125            \n"
          ]
        }
      ],
      "source": [
        "# list datasets on kaggle with the name Ravdess\n",
        "\n",
        "!kaggle datasets list -s RAVDESS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Téléchargement des données sous format csv\n",
        "\n",
        "kaggle.api.dataset_download_files('kartik2khandelwal/speech-emotion-dataset', path='.', unzip=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qdm-fcBGYpe"
      },
      "source": [
        "## Data Processing\n",
        "- Audio Normalization\n",
        "- Silence Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsnJYdGqHD3N"
      },
      "outputs": [],
      "source": [
        "def normalize(X):\n",
        "    max_X = np.max(np.abs(X))\n",
        "    Y = X / max_X\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiPbwP4jHDY_"
      },
      "outputs": [],
      "source": [
        "def remove_silence(X, factor):\n",
        "    max_X = np.max(np.abs(X))\n",
        "    decision_threshold = max_X / factor\n",
        "\n",
        "    # Find the indices of samples above the decision threshold\n",
        "    indices_useful_X, _ = find_peaks(np.abs(X), height=decision_threshold)\n",
        "\n",
        "    # Extract the useful samples\n",
        "    Y = X[indices_useful_X[0]:indices_useful_X[-1] + 1]\n",
        "\n",
        "    return Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "856qfyvJrizG"
      },
      "outputs": [],
      "source": [
        "def normalise_remove_silence(input_filename, output_directory, factor):\n",
        "    # Read the input audio file\n",
        "    x, fs = librosa.load(input_filename, sr=None)\n",
        "\n",
        "    # Normalize the audio\n",
        "    y = normalize(x)\n",
        "\n",
        "    # Remove silence\n",
        "    z = remove_silence(y, factor)\n",
        "\n",
        "    # Extract the filename and extension\n",
        "    filename, extension = os.path.splitext(os.path.basename(input_filename))\n",
        "\n",
        "    # Create the output filename in the specified directory\n",
        "    output_filename = os.path.join(output_directory, f\"{filename}_N_RS{factor}{extension}\")\n",
        "\n",
        "    # Write the processed audio to the output file\n",
        "    sf.write(output_filename, z, fs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x461Ay9HL3N"
      },
      "source": [
        "## Audio Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwaE8Cnw4Ec_"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "def global_feature_computation(feature_matrix, computations):\n",
        "    computed_features = []\n",
        "    for computation in computations:\n",
        "        if computation == \"mean\":\n",
        "            computed_features.append(np.mean(feature_matrix, axis=1))\n",
        "        elif computation == \"min\":\n",
        "            computed_features.append(np.min(feature_matrix, axis=1))\n",
        "        elif computation == \"max\":\n",
        "            computed_features.append(np.max(feature_matrix, axis=1))\n",
        "        elif computation == \"std\":\n",
        "            computed_features.append(np.std(feature_matrix, axis=1))\n",
        "        elif computation == \"range\":\n",
        "            computed_features.append(np.ptp(feature_matrix, axis=1))\n",
        "        elif computation == \"mode\":\n",
        "            # Use scipy.stats.mode to get mode and count\n",
        "            mode_result = mode(feature_matrix, axis=1)\n",
        "            computed_features.append(mode_result.mode.flatten())\n",
        "        elif computation == \"median\":\n",
        "            computed_features.append(np.median(feature_matrix, axis=1))\n",
        "        elif computation == \"1st_quartile\":\n",
        "            computed_features.append(np.percentile(feature_matrix, 25, axis=1))\n",
        "        elif computation == \"3rd_quartile\":\n",
        "            computed_features.append(np.percentile(feature_matrix, 75, axis=1))\n",
        "        # Add conditions for other computations\n",
        "\n",
        "    return np.concatenate(computed_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDTlQUWwGMwh"
      },
      "outputs": [],
      "source": [
        "def extract_features(audio_path, features, global_computation):\n",
        "    # Load the normalized and silence-removed audio\n",
        "    audio, sr = librosa.load(audio_path)\n",
        "\n",
        "    lst_spectral_flatness = []\n",
        "    lst_spectral_centroid = []\n",
        "    lst_mfcc = []\n",
        "    lst_melspectrogram = []\n",
        "    lst_chroma_stft = []\n",
        "    lst_rms = []\n",
        "\n",
        "    feature_list = []\n",
        "\n",
        "    # Extract selected features\n",
        "    for feature_name in features:\n",
        "        if feature_name == \"spectral_flatness\":\n",
        "            spectral_flatness = librosa.feature.spectral_flatness(y=audio)\n",
        "            lst_spectral_flatness.append(global_feature_computation(spectral_flatness, global_computation))\n",
        "        elif feature_name == \"spectral_centroid\":\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr = sr)\n",
        "            lst_spectral_centroid.append(global_feature_computation(spectral_centroid, global_computation))\n",
        "        elif feature_name == \"mfcc\":\n",
        "            mfcc = librosa.feature.mfcc(y=audio, sr = sr)\n",
        "            lst_mfcc.append(global_feature_computation(mfcc, global_computation))\n",
        "        elif feature_name == \"melspectrogram\":\n",
        "            mel_spectrum = librosa.feature.melspectrogram(y=audio, sr = sr)\n",
        "            lst_melspectrogram.append(global_feature_computation(mel_spectrum, global_computation))\n",
        "        elif feature_name == \"chroma_stft\":\n",
        "            chroma_stft = librosa.feature.chroma_stft(y=audio, sr = sr)\n",
        "            lst_chroma_stft.append(global_feature_computation(chroma_stft, global_computation))\n",
        "        elif feature_name == \"rms\":\n",
        "            rms = librosa.feature.rms(y=audio)\n",
        "            lst_rms.append(global_feature_computation(rms, global_computation))\n",
        "\n",
        "\n",
        "    feature_list.append(lst_spectral_flatness[0])\n",
        "    feature_list.append(lst_spectral_centroid[0])\n",
        "    feature_list.append(lst_mfcc[0])\n",
        "    feature_list.append(lst_melspectrogram[0])\n",
        "    feature_list.append(lst_chroma_stft[0])\n",
        "    feature_list.append(lst_rms[0])\n",
        "\n",
        "\n",
        "    # Combine the extracted features into a single feature vector\n",
        "    feature_vector = np.concatenate(feature_list)\n",
        "\n",
        "    return feature_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8lELnXZLGMwh"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEvoQeJHTuM"
      },
      "source": [
        "## Model Training\n",
        "- SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1ESsHiRCvf5"
      },
      "outputs": [],
      "source": [
        "features = [\"spectral_flatness\", \"spectral_centroid\", \"mfcc\", \"chroma_stft\", \"melspectrogram\", \"rms\"]\n",
        "global_computation = [\"mean\"]\n",
        "#global_computation = [\"mean\", \"min\", \"max\", \"std\", \"range\", \"mode\", \"median\"]\n",
        "\n",
        "class_names = [\"Neutral\", \"Cal\", \"Happiness\", \"Sadness\", \"Angry\", \"Fear\", \"Disgust\", \"Surprise\"]\n",
        "\n",
        "factor = 200\n",
        "\n",
        "num_selected_features = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJs_JWYnJMRC"
      },
      "source": [
        "### Normalizing and removing the silence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-k4x5T1CXhl"
      },
      "outputs": [],
      "source": [
        "# Set the base directory where actor directories are located\n",
        "base_directory = '/content/audio_speech_actors_01-24'\n",
        "output_directory = '/content/processed_data'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# First loop: Normalize and remove silence\n",
        "for actor_directory in os.listdir(base_directory):\n",
        "    actor_path = os.path.join(base_directory, actor_directory)\n",
        "\n",
        "    # Check if it's a directory\n",
        "    if os.path.isdir(actor_path):\n",
        "\n",
        "        # Iterate over audio files in the actor's directory\n",
        "        for audio_file in os.listdir(actor_path):\n",
        "            audio_path = os.path.join(actor_path, audio_file)\n",
        "\n",
        "            # Normalize and remove silence\n",
        "            normalise_remove_silence(audio_path, output_directory, factor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5Vo0jRWJJps"
      },
      "outputs": [],
      "source": [
        "# List to store the paths of normalized audio files\n",
        "normalized_audio_paths = []\n",
        "\n",
        "for audio_file in os.listdir(output_directory):\n",
        "    normalized_audio_path = os.path.join(output_directory, audio_file)\n",
        "\n",
        "    # Append to the list of normalized audio paths\n",
        "    normalized_audio_paths.append(normalized_audio_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1YrGP83J0-7"
      },
      "outputs": [],
      "source": [
        "# Initialize lists for audio files, labels, and features\n",
        "features_list = []\n",
        "audio_files = []\n",
        "\n",
        "for normalized_audio_path in normalized_audio_paths:\n",
        "    extracted_features = extract_features(normalized_audio_path, features, global_computation)\n",
        "\n",
        "    # Append to lists or perform further processing as needed\n",
        "    audio_files.append(normalized_audio_path)\n",
        "    features_list.append(extracted_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma06Q3jPTvPL"
      },
      "outputs": [],
      "source": [
        "# Convert features_list to a NumPy array if needed\n",
        "features_list = np.array(features_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvO6Qr65dgYm",
        "outputId": "4b2acc96-69e2-4393-ee53-3577169a5e41"
      },
      "outputs": [],
      "source": [
        "features_list.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyPugdT1ptV6"
      },
      "source": [
        "### labels\n",
        "Here is the filename identifiers as per the official RAVDESS website:\n",
        "\n",
        "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "- Vocal channel (01 = speech, 02 = song).\n",
        "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
        "- Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "\n",
        "So, here's an example of an audio filename. 02-01-06-01-02-01-12.mp4 This means the meta data for the audio file is:\n",
        "\n",
        "- Video-only (02)\n",
        "- Speech (01)\n",
        "- Fearful (06)\n",
        "- Normal intensity (01)\n",
        "- Statement \"dogs\" (02)\n",
        "- 1st Repetition (01)\n",
        "- 12th Actor (12) - Female (as the actor ID number is even)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EqQ9fUPpsto"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "\n",
        "for actor_directory in os.listdir(base_directory):\n",
        "    actor_path = os.path.join(base_directory, actor_directory)\n",
        "\n",
        "     #Check if it's a directory\n",
        "    if os.path.isdir(actor_path):\n",
        "\n",
        "        # Iterate over audio files in the actor's directory\n",
        "        for audio_file in os.listdir(actor_path):\n",
        "            # Split the file name using the \"-\" delimiter and take the third part as the emotion label\n",
        "            parts = audio_file.split(\"-\")\n",
        "            emotion_label = parts[2]\n",
        "            labels.append(emotion_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzJGBcOnrhdB"
      },
      "outputs": [],
      "source": [
        "labels = np.array(labels)\n",
        "labels = labels.astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEWFQ0DLrSH_",
        "outputId": "586af70c-242d-4a38-eb28-414f4c58d869"
      },
      "outputs": [],
      "source": [
        "labels.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading the csv of RAVDESS from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-637.701233</td>\n",
              "      <td>104.299019</td>\n",
              "      <td>4.894947</td>\n",
              "      <td>20.494011</td>\n",
              "      <td>12.552954</td>\n",
              "      <td>2.851410</td>\n",
              "      <td>-6.633390</td>\n",
              "      <td>-4.091278</td>\n",
              "      <td>-10.423918</td>\n",
              "      <td>-6.406950</td>\n",
              "      <td>...</td>\n",
              "      <td>0.172893</td>\n",
              "      <td>-1.170210</td>\n",
              "      <td>-5.292450</td>\n",
              "      <td>-0.573319</td>\n",
              "      <td>1.019471</td>\n",
              "      <td>-3.492607</td>\n",
              "      <td>-3.468123</td>\n",
              "      <td>-1.214944</td>\n",
              "      <td>1.971239</td>\n",
              "      <td>male_calm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-596.908460</td>\n",
              "      <td>86.871936</td>\n",
              "      <td>9.470162</td>\n",
              "      <td>17.109819</td>\n",
              "      <td>11.198966</td>\n",
              "      <td>1.541056</td>\n",
              "      <td>-6.677264</td>\n",
              "      <td>-5.755428</td>\n",
              "      <td>-9.684472</td>\n",
              "      <td>-6.891256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033803</td>\n",
              "      <td>-1.986515</td>\n",
              "      <td>-5.103855</td>\n",
              "      <td>-1.253110</td>\n",
              "      <td>0.514896</td>\n",
              "      <td>-3.268317</td>\n",
              "      <td>-4.502895</td>\n",
              "      <td>0.167153</td>\n",
              "      <td>0.256732</td>\n",
              "      <td>male_calm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-698.086548</td>\n",
              "      <td>99.795929</td>\n",
              "      <td>1.892679</td>\n",
              "      <td>19.915264</td>\n",
              "      <td>7.532868</td>\n",
              "      <td>1.265761</td>\n",
              "      <td>-9.188656</td>\n",
              "      <td>-5.798194</td>\n",
              "      <td>-12.299710</td>\n",
              "      <td>-4.976400</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.639542</td>\n",
              "      <td>-2.603761</td>\n",
              "      <td>-4.890347</td>\n",
              "      <td>-0.879222</td>\n",
              "      <td>-1.250208</td>\n",
              "      <td>-3.449960</td>\n",
              "      <td>-4.708529</td>\n",
              "      <td>-0.086224</td>\n",
              "      <td>-3.034044</td>\n",
              "      <td>male_calm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-279.141052</td>\n",
              "      <td>41.092949</td>\n",
              "      <td>-21.319229</td>\n",
              "      <td>7.802911</td>\n",
              "      <td>-13.140503</td>\n",
              "      <td>-9.407660</td>\n",
              "      <td>-15.580647</td>\n",
              "      <td>-6.097223</td>\n",
              "      <td>-24.700903</td>\n",
              "      <td>-9.640293</td>\n",
              "      <td>...</td>\n",
              "      <td>-10.305976</td>\n",
              "      <td>-4.092835</td>\n",
              "      <td>-5.817179</td>\n",
              "      <td>-10.731523</td>\n",
              "      <td>-0.823596</td>\n",
              "      <td>-15.885103</td>\n",
              "      <td>-2.014258</td>\n",
              "      <td>-6.173852</td>\n",
              "      <td>-5.331760</td>\n",
              "      <td>male_angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-160.074686</td>\n",
              "      <td>17.576058</td>\n",
              "      <td>-2.147436</td>\n",
              "      <td>3.133417</td>\n",
              "      <td>-4.745002</td>\n",
              "      <td>-6.510771</td>\n",
              "      <td>-5.911591</td>\n",
              "      <td>-4.481506</td>\n",
              "      <td>-9.470598</td>\n",
              "      <td>-5.907823</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.088007</td>\n",
              "      <td>-1.817639</td>\n",
              "      <td>-2.994765</td>\n",
              "      <td>-4.893176</td>\n",
              "      <td>-2.880436</td>\n",
              "      <td>-7.163400</td>\n",
              "      <td>-2.147345</td>\n",
              "      <td>-2.359248</td>\n",
              "      <td>-3.746217</td>\n",
              "      <td>male_angry</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1          2          3          4         5  \\\n",
              "0 -637.701233  104.299019   4.894947  20.494011  12.552954  2.851410   \n",
              "1 -596.908460   86.871936   9.470162  17.109819  11.198966  1.541056   \n",
              "2 -698.086548   99.795929   1.892679  19.915264   7.532868  1.265761   \n",
              "3 -279.141052   41.092949 -21.319229   7.802911 -13.140503 -9.407660   \n",
              "4 -160.074686   17.576058  -2.147436   3.133417  -4.745002 -6.510771   \n",
              "\n",
              "           6         7          8         9  ...         11        12  \\\n",
              "0  -6.633390 -4.091278 -10.423918 -6.406950  ...   0.172893 -1.170210   \n",
              "1  -6.677264 -5.755428  -9.684472 -6.891256  ...   0.033803 -1.986515   \n",
              "2  -9.188656 -5.798194 -12.299710 -4.976400  ...  -1.639542 -2.603761   \n",
              "3 -15.580647 -6.097223 -24.700903 -9.640293  ... -10.305976 -4.092835   \n",
              "4  -5.911591 -4.481506  -9.470598 -5.907823  ...  -4.088007 -1.817639   \n",
              "\n",
              "         13         14        15         16        17        18        19  \\\n",
              "0 -5.292450  -0.573319  1.019471  -3.492607 -3.468123 -1.214944  1.971239   \n",
              "1 -5.103855  -1.253110  0.514896  -3.268317 -4.502895  0.167153  0.256732   \n",
              "2 -4.890347  -0.879222 -1.250208  -3.449960 -4.708529 -0.086224 -3.034044   \n",
              "3 -5.817179 -10.731523 -0.823596 -15.885103 -2.014258 -6.173852 -5.331760   \n",
              "4 -2.994765  -4.893176 -2.880436  -7.163400 -2.147345 -2.359248 -3.746217   \n",
              "\n",
              "       labels  \n",
              "0   male_calm  \n",
              "1   male_calm  \n",
              "2   male_calm  \n",
              "3  male_angry  \n",
              "4  male_angry  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('features.csv')\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WvZeMYNZKOnD"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(['labels'], axis=1), df['labels'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6mMgE1UkYFh",
        "outputId": "a44221ea-ff22-4c7b-ca9d-e1c860d0fc02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((3456, 20), (3456,), (864, 20), (864,))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "74WDB2gseUHP",
        "outputId": "759f8920-2f4a-4062-833b-a3a4d6db0952"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(max_iter=1000, random_state=42)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize and train the SVM model\n",
        "svc_model = SVC()\n",
        "svc_model.fit(X_train, y_train)\n",
        "#\n",
        "#\n",
        "# Initialize and train the KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "#\n",
        "#\n",
        "# Initialize and train the Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "#\n",
        "#\n",
        "# Standardize the data before training neural networks\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "#\n",
        "# Initialize and train the Neural Network model\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
        "nn_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AEDMrrSrK6Vg"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set\n",
        "y_pred_svc = svc_model.predict(X_test)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "y_pred_nn = nn_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJI5P260hW8S",
        "outputId": "2a2a6f98-a821-4672-dba6-7e30c7124e03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((864,), (864,), (864,), (864,))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred_svc.shape, y_pred_knn.shape,  y_pred_gb.shape, y_pred_nn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kp-McQsOgGFn"
      },
      "outputs": [],
      "source": [
        "# Accuracies\n",
        "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "accuracy_gb  = accuracy_score(y_test, y_pred_gb)\n",
        "accuracy_nn  = accuracy_score(y_test, y_pred_nn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3fioUCxmgiIg"
      },
      "outputs": [],
      "source": [
        "models = []\n",
        "accuracies = []\n",
        "\n",
        "models.append(svc_model)\n",
        "models.append(knn_model)\n",
        "models.append(gb_model)\n",
        "models.append(nn_model)\n",
        "\n",
        "accuracies.append(accuracy_svc)\n",
        "accuracies.append(accuracy_knn)\n",
        "accuracies.append(accuracy_gb)\n",
        "accuracies.append(accuracy_nn)\n",
        "\n",
        "model_accuracy_df = pd.DataFrame(models, columns=['Models'])\n",
        "model_accuracy_df['Accuracies'] = accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "7qVgbqDXi2OV",
        "outputId": "20798447-3faa-48c9-d40c-52fbd3ec5747"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Models</th>\n",
              "      <th>Accuracies</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SVC()</td>\n",
              "      <td>0.237269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KNeighborsClassifier()</td>\n",
              "      <td>0.381944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
              "      <td>0.583333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MLPClassifier(max_iter=1000, random_state=42)</td>\n",
              "      <td>0.495370</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Models  Accuracies\n",
              "0                                              SVC()    0.237269\n",
              "1                             KNeighborsClassifier()    0.381944\n",
              "2  ([DecisionTreeRegressor(criterion='friedman_ms...    0.583333\n",
              "3      MLPClassifier(max_iter=1000, random_state=42)    0.495370"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_accuracy_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqWiAnkbZNbB",
        "outputId": "0abe1db0-b625-4a94-8a27-a5b2066d6448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.24\n",
            "Precision: 0.21\n",
            "Recall: 0.24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/siakesophie/Local_Docs/ING_3/S9/Engineering_project/project/SERAPP/.env-serapp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(svc_model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVnRkUQsZWbS",
        "outputId": "f89b8cae-cac8-4c14-bb62-73b662e103ad"
      },
      "outputs": [],
      "source": [
        "evaluate_model(knn_model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImMK-WmwZYbt",
        "outputId": "9dae976b-8d79-445b-c703-12dd2311bc13"
      },
      "outputs": [],
      "source": [
        "evaluate_model(gb_model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAH-qoZ1ZY4l",
        "outputId": "d918cff0-04eb-4b1a-9225-ecb2fb44cd0a"
      },
      "outputs": [],
      "source": [
        "evaluate_model(nn_model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4VXWFX4TZI4"
      },
      "source": [
        "## Training with DNN & CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWTCTvSxTYt9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFDiInOpXVCa"
      },
      "outputs": [],
      "source": [
        "Y = []\n",
        "for i in range(labels.size):\n",
        "  Y.append(labels[i])\n",
        "  print(labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "zbwLauNWLEE_",
        "outputId": "85121589-d5e6-482b-c2b6-ec47256f05f9"
      },
      "outputs": [],
      "source": [
        "# Reconstruction of the dataset with the lables from 01 to 07\n",
        "\n",
        "display(features_list.shape)\n",
        "df = pd.DataFrame(features_list)\n",
        "df['labels'] = Y\n",
        "df.to_csv('features.csv', index=False)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "_8agDCOaOGOk",
        "outputId": "a747df1f-a4ac-4cb1-b2b7-d7244f989e29"
      },
      "outputs": [],
      "source": [
        "df_norm = df.drop(\"labels\", axis=1)\n",
        "df_norm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qnkNK-INh6m"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df_norm)\n",
        "print(scaler.mean_)\n",
        "df_norm = scaler.transform(df_norm)\n",
        "\n",
        "df_norm = pd.DataFrame(df_norm)\n",
        "df_norm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksBMLuWDQHj1"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_norm, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXYasv_IZ1oL",
        "outputId": "e00b4d99-1093-4803-9b63-d3973b54b5f0"
      },
      "outputs": [],
      "source": [
        "# Let check the data shape\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBU7PyVHaZKG",
        "outputId": "3cbb20e9-042b-4a7f-882a-d2333824d2d9"
      },
      "outputs": [],
      "source": [
        "# For use the CNN model, let expand the data dimensions\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8QuPWzrar3s"
      },
      "outputs": [],
      "source": [
        "# Modeling\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv1D(128, kernel_size=5, strides = 1, padding='same', activation=tf.nn.relu, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation=tf.nn.relu))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=32, activation=tf.nn.relu))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(units=9, activation=tf.nn.softmax))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgtMexw0bE9D"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPrFCHZMbUey"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor  = \"val_accuracy\",\n",
        "                   mode     = 'max',\n",
        "                   verbose  = 0,\n",
        "                   patience = 30)\n",
        "mc = ModelCheckpoint('model_best.h5',\n",
        "                     monitor        = 'val_accuracy',\n",
        "                     mode           = 'max',\n",
        "                     verbose        = 1,\n",
        "                     save_best_only = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo1rt14Ra5KO",
        "outputId": "5425538b-b26b-4158-c025-eee5ea860ebf"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRmVz95abaTj",
        "outputId": "f4807836-23d5-4adc-c996-8a7220271134"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 100\n",
        "EPOCHS = 30\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, callbacks=[es, mc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "pmdVnuAXg_mc",
        "outputId": "b7cdec49-395d-428e-eb85-2b2f03a357e2"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of our model on test data : \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")\n",
        "\n",
        "epochs = [i for i in range(EPOCHS)]\n",
        "fig , ax = plt.subplots(1,2)\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "test_acc = history.history['val_accuracy']\n",
        "test_loss = history.history['val_loss']\n",
        "\n",
        "fig.set_size_inches(20,6)\n",
        "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
        "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
        "ax[0].set_title('Training & Testing Loss')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "\n",
        "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
        "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
        "ax[1].set_title('Training & Testing Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
